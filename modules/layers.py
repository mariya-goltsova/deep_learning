import numpy as np
from typing import List
from .base import Module


class Linear(Module):
    """
    Applies linear (affine) transformation of data: y = x W^T + b
    """
    def __init__(self, in_features: int, out_features: int, bias: bool = True):
        """
        :param in_features: input vector features
        :param out_features: output vector features
        :param bias: whether to use additive bias
        """
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = np.random.uniform(-1, 1, (out_features, in_features)) / np.sqrt(in_features)
        self.bias = np.random.uniform(-1, 1, out_features) / np.sqrt(in_features) if bias else None

        self.grad_weight = np.zeros_like(self.weight)
        self.grad_bias = np.zeros_like(self.bias) if bias else None

    def compute_output(self, input: np.ndarray) -> np.ndarray:
        """
        :param input: array of shape (batch_size, in_features)
        :return: array of shape (batch_size, out_features)
        """
        # replace with your code ｀、ヽ｀、ヽ(ノ＞＜)ノ ヽ｀☂｀、ヽ
        # return super().compute_output(input)
        if self.bias is not None:
          return input @ self.weight.T + self.bias
        else:
          return input @ self.weight.T

    def compute_grad_input(self, input: np.ndarray, grad_output: np.ndarray) -> np.ndarray:
        """
        :param input: array of shape (batch_size, in_features)
        :param grad_output: array of shape (batch_size, out_features)
        :return: array of shape (batch_size, in_features)
        """
        # replace with your code ｀、ヽ｀、ヽ(ノ＞＜)ノ ヽ｀☂｀、ヽ
        # return super().compute_grad_input(input, grad_output)
        return grad_output @ self.weight

    def update_grad_parameters(self, input: np.ndarray, grad_output: np.ndarray):
        """
        :param input: array of shape (batch_size, in_features)
        :param grad_output: array of shape (batch_size, out_features)
        """
        # replace with your code ｀、ヽ｀、ヽ(ノ＞＜)ノ ヽ｀☂｀、ヽ
        # super().update_grad_parameters(input, grad_output)
        # print(input.shape)
        # print(self.weight.shape)
        # print(grad_output.shape)
        self.grad_weight += grad_output.T @ input
        if self.grad_bias is not None:
          self.grad_bias += grad_output.sum(axis=0)
        else:
          self.grad_bias = grad_output.sum(axis=0)

    def zero_grad(self):
        self.grad_weight.fill(0)
        if self.bias is not None:
            self.grad_bias.fill(0)

    def parameters(self) -> List[np.ndarray]:
        if self.bias is not None:
            return [self.weight, self.bias]

        return [self.weight]

    def parameters_grad(self) -> List[np.ndarray]:
        if self.bias is not None:
            return [self.grad_weight, self.grad_bias]

        return [self.grad_weight]

    def __repr__(self) -> str:
        out_features, in_features = self.weight.shape
        return f'Linear(in_features={in_features}, out_features={out_features}, ' \
               f'bias={not self.bias is None})'


class BatchNormalization(Module):
    """
    Applies batch normalization transformation
    """
    def __init__(self, num_features: int, eps: float = 1e-5, momentum: float = 0.1, affine: bool = True):
        """
        :param num_features:
        :param eps:
        :param momentum:
        :param affine: whether to use trainable affine parameters
        """
        super().__init__()
        self.eps = eps
        self.momentum = momentum
        self.affine = affine

        self.running_mean = np.zeros(num_features)
        self.running_var = np.ones(num_features)

        self.weight = np.ones(num_features) if affine else None
        self.bias = np.zeros(num_features) if affine else None

        self.grad_weight = np.zeros_like(self.weight) if affine else None
        self.grad_bias = np.zeros_like(self.bias) if affine else None

        # store this values during forward path and re-use during backward pass
        self.mean = None
        self.input_mean = None  # input - mean
        self.var = None
        self.sqrt_var = None
        self.inv_sqrt_var = None
        self.norm_input = None

    def compute_output(self, input: np.ndarray) -> np.ndarray:
        """
        :param input: array of shape (batch_size, num_features)
        :return: array of shape (batch_size, num_features)
        """
        # replace with your code ｀、ヽ｀、ヽ(ノ＞＜)ノ ヽ｀☂｀、ヽ
        # return super().compute_output(input)

        if self.training:
            # train mode
            self.mean = np.mean(input, axis=0)
            self.input_mean = input - self.mean
            self.var = np.var(input, axis=0)
            self.sqrt_var = np.sqrt(self.var + self.eps)
            self.inv_sqrt_var = 1 / self.sqrt_var
            self.norm_input = self.input_mean * self.inv_sqrt_var

            if self.affine:
                y = self.norm_input * self.weight + self.bias
            else:
                y = self.norm_input

            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * self.mean
            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * self.var * (input.shape[0] / (input.shape[0]-1))
            
        else:
            # eval mode
            input_ = (input - self.running_mean) / np.sqrt(self.running_var + self.eps)
            if self.affine:
                y = input_ * self.weight + self.bias
            else:
                y = input_
        return y


    def compute_grad_input(self, input: np.ndarray, grad_output: np.ndarray) -> np.ndarray:
        """
        это dL/dx
        :param input: array of shape (batch_size, num_features)
        :param grad_output: array of shape (batch_size, num_features)
        :return: array of shape (batch_size, num_features)

        'Это помогло мне разобраться по чему надо брать производные. Взято из ГПТ:
        forward:
                    x
            │
            ├──→ μ = mean(x)
            │
            ├──→ x_centered = x - μ
            │
            ├──→ var = mean(x_centered²) = sum((x - mean)**2) / b
            │
            ├──→ std = sqrt(var + eps)
            │
            ├──→ inv_std = 1 / std = 1 / sqrt(mean(x_centered²) + eps)
            │
            ├──→ x_hat = x_centered * inv_std
            │
            └──→ y = γ * x_hat + β
                    ↓
                Loss
        """
        # replace with your code ｀、ヽ｀、ヽ(ノ＞＜)ノ ヽ｀☂｀、ヽ
        # return super().compute_grad_input(input, grad_output)
        if not self.training:
            inv_std = 1.0 / np.sqrt(self.running_var + self.eps)

            if self.affine:
                return grad_output * self.weight * inv_std
            else:
                return grad_output * inv_std
        
        if self.affine:
            d_norm_inp = grad_output * self.weight if self.affine else grad_output
        else:
            d_norm_inp = grad_output

        d_inv_var = np.sum(d_norm_inp * self.input_mean * -.5 * self.inv_sqrt_var ** 3, axis=0)
        d_input_mean = np.sum(d_norm_inp * (-self.inv_sqrt_var), 0) + \
            np.mean(-2 * self.input_mean, axis=0) * d_inv_var


        res = d_norm_inp * self.inv_sqrt_var + \
            d_inv_var * 2 * self.input_mean / input.shape[0] + \
            d_input_mean / input.shape[0] 

        return  res


    def update_grad_parameters(self, input: np.ndarray, grad_output: np.ndarray):
        """
        :param input: array of shape (batch_size, num_features)
        :param grad_output: array of shape (batch_size, num_features)
        """
        # replace with your code ｀、ヽ｀、ヽ(ノ＞＜)ノ ヽ｀☂｀、ヽ
        # super().update_grad_parameters(input, grad_output)
        if not self.affine:
          return

        if not self.training:
            x_norm = (input - self.running_mean) / np.sqrt(self.running_var + self.eps)
        else:
            x_norm = self.norm_input
        self.grad_weight += np.sum(grad_output * x_norm, axis=0)
        self.grad_bias += grad_output.sum(axis=0)

    def zero_grad(self):
        if self.affine:
            self.grad_weight.fill(0)
            self.grad_bias.fill(0)

    def parameters(self) -> List[np.ndarray]:
        return [self.weight, self.bias] if self.affine else []

    def parameters_grad(self) -> List[np.ndarray]:
        return [self.grad_weight, self.grad_bias] if self.affine else []

    def __repr__(self) -> str:
        return f'BatchNormalization(num_features={len(self.running_mean)}, ' \
               f'eps={self.eps}, momentum={self.momentum}, affine={self.affine})'


class Dropout(Module):
    """
    Applies dropout transformation
    """
    def __init__(self, p=0.5):
        super().__init__()
        assert 0 <= p < 1
        self.p = p
        self.mask = None

    def compute_output(self, input: np.ndarray) -> np.ndarray:
        """
        :param input: array of an arbitrary size
        :return: array of the same size
        """
        # replace with your code ｀、ヽ｀、ヽ(ノ＞＜)ノ ヽ｀☂｀、ヽ
        # return super().compute_output(input)
        if self.training:
            self.mask = np.random.binomial(1, 1-self.p, size=input.shape)
            return 1 / (1-self.p) * self.mask * input
        else:
            return input
        

    def compute_grad_input(self, input: np.ndarray, grad_output: np.ndarray) -> np.ndarray:
        """
        :param input: array of an arbitrary size
        :param grad_output: array of the same size
        :return: array of the same size
        """
        # replace with your code ｀、ヽ｀、ヽ(ノ＞＜)ノ ヽ｀☂｀、ヽ
        # return super().compute_grad_input(input, grad_output)
        if self.training:
            y = grad_output / (1-self.p) * self.mask
            return y
        else:
            return grad_output

    def __repr__(self) -> str:
        return f'Dropout(p={self.p})'


class Sequential(Module):
    """
    Container for consecutive application of modules
    """
    def __init__(self, *args):
        super().__init__()
        self.modules = list(args)

    def compute_output(self, input: np.ndarray) -> np.ndarray:
        """
        :param input: array of size matching the input size of the first layer
        :return: array of size matching the output size of the last layer
        """
        # replace with your code ｀、ヽ｀、ヽ(ノ＞＜)ノ ヽ｀☂｀、ヽ
        # return super().compute_output(input)
        self.inputs = []
        for module in self.modules:
            self.inputs.append(input)
            input = module.forward(input)
        return input       

    def compute_grad_input(self, input: np.ndarray, grad_output: np.ndarray) -> np.ndarray:
        """
        :param input: array of size matching the input size of the first layer
        :param grad_output: array of size matching the output size of the last layer
        :return: array of size matching the input size of the first layer
        """
        # replace with your code ｀、ヽ｀、ヽ(ノ＞＜)ノ ヽ｀☂｀、ヽ
        # return super().compute_grad_input(input, grad_output)
        for i in np.arange(len(self.modules)-1, -1, -1):
            grad_output = self.modules[i].backward(self.inputs[i], grad_output)
        return grad_output

    def __getitem__(self, item):
        return self.modules[item]

    def train(self):
        for module in self.modules:
            module.train()

    def eval(self):
        for module in self.modules:
            module.eval()

    def zero_grad(self):
        for module in self.modules:
            module.zero_grad()

    def parameters(self) -> List[np.ndarray]:
        return [parameter for module in self.modules for parameter in module.parameters()]

    def parameters_grad(self) -> List[np.ndarray]:
        return [grad for module in self.modules for grad in module.parameters_grad()]

    def __repr__(self) -> str:
        repr_str = 'Sequential(\n'
        for module in self.modules:
            repr_str += ' ' * 4 + repr(module) + '\n'
        repr_str += ')'
        return repr_str
